# -*- coding: utf-8 -*-
"""prediksi model lapisan 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iC9MYo4IlHgerB6pH23nnrwtV_zDL3kA
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from google.colab import drive
drive.mount('/content/drive/')

df = pd.read_csv ('/content/drive/MyDrive/skripsi/datasetXAUUSD.csv')

#melihat info data yang sudah diubah tipedata
df.info()

#menghapus fitur volume
df = df.drop(columns=['Vol.'])

#mengubah tepidata menjadi datatime
df['Date'] = pd.to_datetime(df['Date'])

#mengubah tipedata menjadi float
df['Price'] = df['Price'].str.replace(',', '').astype(float)

#mengubah tipedata menjadi float
df['Open'] = df['Open'].str.replace(',', '').astype(float)

#mengubah tipedata menjadi float
df['High'] = df['High'].str.replace(',', '').astype(float)

#mengubah tipedata menjadi float
df['Low'] = df['Low'].str.replace(',', '').astype(float)

# Menghapus simbol '%' dan mengubah tipe data menjadi float
df['Change %'] = df['Change %'].str.replace('%', '').str.replace(',', '').astype(float)

df.info()

# Menghitung matriks korelasi
correlation_matrix = df.drop(columns=["Date"]).corr()

# Menampilkan matriks korelasi
plt.figure(figsize=(7, 5))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
plt.title("Correlation Matrix")
plt.show()

# Menentukan fitur target yang potensial
# Kriteria: Fitur yang memiliki korelasi tinggi dengan sebagian besar fitur lainnya
correlation_sum = correlation_matrix.sum(axis=0) - 1  # Menghitung jumlah korelasi (mengabaikan dirinya sendiri)
target_candidate = correlation_sum.idxmax()  # Fitur dengan total korelasi tertinggi
print("Fitur target yang paling potensial berdasarkan hubungan dengan fitur lain adalah:", target_candidate)
# Menampilkan skor korelasi untuk setiap fitur
print("\nTotal skor korelasi setiap fitur:")
print(correlation_sum)

#mengambil fitur Date sebagai indeks dan fitur Price sebagai target
df = df [['Date', 'Price']].set_index('Date')

#melihat fitur yang diambil
print(df)

#menampilkan visualisasi price XAU/USD
plt.style.use('default')
plt.figure(figsize=(16,5))
plt.title('Harga XAU/USD 5 tahun terakhir')
plt.plot( df['Price'], linewidth=1.2)
plt.xlabel('Tanggal')
plt.ylabel('dollar(usd)')
plt.xticks(rotation=45)
plt.grid(True,linewidth=0.18, axis='y')
plt.show()

sc = MinMaxScaler(feature_range=(0, 1))  #Normalisasi untuk mengubah nilai dalam rentang antara 0 dan 1
data = df['Price'].values.reshape(-1, 1) #Mengambil data dari kolom Price dan Mengubah array 1 dimensi menjadi array 2 dimensi (dengan bentuk [n_samples, 1])
scaled_data = sc.fit_transform(data)     # Melakukan normalisasi pada data

# Mengubah array hasil normalisasi (scaled_data) menjadi sebuah DataFrame dan Memberikan nama kolom baru, yaitu "Normalisasi (Price)", untuk hasil normalisasi.
scaled_df = pd.DataFrame(scaled_data, index=df.index, columns=['Normalisasi (Price)'])
print(scaled_df)

training_data_len = int(len(data) * 0.8)         #Mengambil 80% dari total data untuk digunakan sebagai data pelatihan.
train_data = scaled_data[:training_data_len, :] #Membagi dataset yang telah dinormalisasi sebelumnya (dengan MinMaxScaler)
x_train = []#Akan menyimpan input untuk model LSTM
y_train = []#Akan menyimpan target output (label) yang diprediksi oleh model

#Menggunkan loop untuk embentuk dataset dengan sliding window dan Mengambil 60 data sebelumnya dari data pelatihan
#Data ini dimasukkan ke dalam x_train sebagai satu set input dan y_train sebagai target output
for i in range(60, len(train_data)):
    x_train.append(train_data[i-60:i, 0])
    y_train.append(train_data[i, 0])

x_train, y_train = np.array(x_train), np.array(y_train)#Mengubah data x_train dan y_train ke array NumPy untuk digunakan dalam model
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))#Mengubah bentuk data input (x_train) menjadi format 3D [samples, time_steps, features], yang diperlukan untuk melatih model LSTM.

from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.metrics import mean_squared_error,r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential

# Pembangunan Model LSTM
model = Sequential()

model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(Dropout(0.2))

model.add(LSTM(units=50))
model.add(Dropout(0.2))

model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, shuffle=False)

# Plot Learning Curve dengan ukuran font yang ditambahkan
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Learning Curve', fontsize=20)
plt.xlabel('Epochs', fontsize=16)
plt.ylabel('Loss', fontsize=16)
plt.legend(fontsize=14)
plt.grid(True)
plt.show()

test_data = scaled_data[training_data_len - 60:, :] #sebagai input prediksi data uji dapat dimulai dengan konteks historis 60 data terakhir dari scaled_data
x_test = [] #data input untuk prediksi
y_test = data[training_data_len:, :] #Target data aktual digunakan untuk evaluasi hasil prediksi.
# Membuat data input uji dengan sliding window
for i in range(60, len(test_data)):
    x_test.append(test_data[i-60:i, 0])

x_test = np.array(x_test) #Mengubah ke Array NumPy agar dapat digunakan dalam model
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1)) #Mengubah bentuk x_test menjadi [samples, time steps, features]

predictions = model.predict(x_test) #Menggunakan model LSTM yang sudah dilatih untuk memprediksi data uji
predictions = sc.inverse_transform(predictions) #Mengubah prediksi yang sebelumnya dinormalisasi (skala 0-1) kembali ke skala aslinya (harga sebenarnya)

valid = pd.DataFrame() #Membuat DataFrame kosong untuk menyimpan data perbandingan antara harga aktual dan hasil prediksi.
valid['Real Price'] = df['Price'][training_data_len:].reset_index(drop=True) #Mengambil kolom harga aktual ("Price") dari DataFrame asli (df).
valid['Predictions'] = predictions #Hasil prediksi dari model LSTM yang sudah diubah ke skala harga asli dan menyimpan nilai prediksi yang akan dibandingkan dengan harga aktual
print(valid)

# Membuat DataFrame untuk perbandingan prediksi dan harga aktual
valid = pd.DataFrame()
valid['Real Price'] = df['Price'][training_data_len:]  # Ambil harga aktual yang sesuai
valid['Predictions'] = predictions
# Visualisasi perbandingan
plt.figure(figsize=(14, 7))
plt.title('Perbandingan Harga Aktual dan Prediksi', fontsize=20)  # Ukuran font judul
plt.xlabel('Tanggal', fontsize=16)  # Ukuran font label x
plt.ylabel('Harga Penutupan (USD)', fontsize=16)  # Ukuran font label y
plt.plot(valid['Real Price'], color='blue', label='Harga Aktual')
plt.plot(valid['Predictions'], color='orange', label='Prediksi')
plt.legend(fontsize=14)  # Ukuran font legend
plt.show()

train = df[:training_data_len]
valid = df[training_data_len:]
valid['Predictions'] = predictions
# Visualisasi dengan ukuran font yang ditambahkan
plt.figure(figsize=(14, 7))
plt.title('Model LSTM untuk Prediksi XAU/USD', fontsize=20)
plt.xlabel('Tanggal', fontsize=16)
plt.ylabel('Harga Penutupan USD', fontsize=16)
# Plot data train
plt.plot(train['Price'], color='green', label='Train')
# Plot data validasi dan prediksi dengan warna yang berbeda
plt.plot(valid['Price'], color='blue', label='Valid')
plt.plot(valid['Predictions'], color='orange', label='Predictions')
plt.legend(loc='lower right', fontsize=14)
plt.grid(True)
plt.show()

#rmse evaluasi menghitung model prediksi bekerja dibandingkan dengan data aktual.
rmse = np.sqrt(mean_squared_error(valid['Price'], valid['Predictions']))
print('Root Mean Squared Error:', rmse)

# R-squared mengukur seberapa baik model prediksi cocok dengan data aktual
r2 = r2_score(valid['Price'],valid['Predictions'])
print('R-squared:', r2)

import pickle

# Simpan model dan objek lainnya
model.save('model_lstm.h5')

with open('scaler.pkl', 'wb') as f:
    pickle.dump(sc, f)

df.to_pickle('dataframe.pkl')

import numpy as np
import pandas as pd
from datetime import timedelta

# Asumsikan model telah dilatih dan scaler telah diinisialisasi
# Ambil 60 data terakhir dari data yang sudah dinormalisasi (scaled_data)
last_60_days = scaled_data[-60:].reshape((1, 60, 1))  # Format: (samples, time_steps, features)

# Prediksi 10 hari ke depan
predictions_10_days = []
for _ in range(10):
    pred = model.predict(last_60_days)  # Prediksi satu langkah ke depan
    predictions_10_days.append(pred[0, 0])  # Simpan prediksi
    # Update input dengan menambahkan prediksi terbaru, buang nilai terlama
    last_60_days = np.append(last_60_days[:, 1:, :], pred.reshape(1, 1, 1), axis=1)

# Kembalikan prediksi ke skala asli
predictions_10_days = sc.inverse_transform(np.array(predictions_10_days).reshape(-1, 1))

# Buat daftar tanggal untuk prediksi 10 hari ke depan
start_date = pd.to_datetime(df.index[-1])  # Tanggal terakhir di dataset asli
future_dates = [start_date + timedelta(days=i) for i in range(1, 11)]

# Buat DataFrame untuk hasil prediksi
result_df = pd.DataFrame({
    'Date': [date.strftime('%Y-%m-%d') for date in future_dates],  # Format tanggal
    'Prediksi (Price)': predictions_10_days.flatten()  # Prediksi harga
})

# Tampilkan hasil prediksi
print(result_df)